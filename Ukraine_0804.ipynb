{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38102d51",
   "metadata": {},
   "source": [
    "# Data Science for Social Justice Workshop Group Project: Ukraine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b7dea",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8addebb",
   "metadata": {},
   "source": [
    "### 1-1. Importing Data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6d530-f95c-40f1-8524-9c15d99a86cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pwd ##it will be different for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbafc694-fd59-40ee-af9c-f872850acc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d422be",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data')  ##it will be different for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92a399-4457-4235-a939-2981f2d77d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7167182c-c93f-4ce5-b175-f9f25142ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c57649-1694-453d-9f34-3512c7a49618",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3649eed0-ca67-4470-93a7-0d1bf100210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18 variables\n",
    "list (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4b1c6-6236-4b12-8e6d-e6bdeecb9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3a019",
   "metadata": {},
   "source": [
    "### 1-2. Dropping Columns and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020d763-032a-488b-93ee-2c9b767bdb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove some columns that we are not going to use\n",
    "\n",
    "df = df.drop(['self', 'url', 'subreddit', 'augmented_at', 'augmented_count'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58de402-9727-4941-8f00-769cdc4b8699",
   "metadata": {},
   "outputs": [],
   "source": [
    "##get rid of posts that have been deleted/removed\n",
    "\n",
    "df = df.loc[~df['selftext'].isin(['[removed]', '[deleted]' ]),:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43e9a1-be78-413f-b7a0-05835e4bce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop null values\n",
    "df = df.dropna(subset=['selftext'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b517ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cfd020-a628-46a8-898f-8291adeb63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove posts in Russian\n",
    "df = df[df['selftext'].map(lambda x: x.isascii())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad10d9-a4cf-43c0-b454-4819d9d6a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e380c-c269-4b46-8fbb-50aa6a57e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bda116-f08a-4504-807e-d8244e944648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of unique users\n",
    "print (df.iloc[:,4].nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e48fc0e-abdc-469b-877f-0d36c138036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency by user\n",
    "frequency = df['author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f5755-2e69-4d14-aed3-ecb063f3bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 50\n",
    "frequency.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418adb5-23ad-4342-b487-5ceff3fca2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean \n",
    "frequency.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9bce9-9460-442c-be47-af659dc458f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##check\n",
    "12421/8723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a467894d-e4ed-47c0-90de-8b7bc2a104d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#average number of words in selftext\n",
    "count = df['selftext'].str.split().str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a483c14-347c-477d-b303-14946b601244",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dee0b1-85c5-4220-911f-97679890389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7482e5",
   "metadata": {},
   "source": [
    "### 1-3. Cleaning Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f15a8e-4dc2-4c70-8aeb-d6156d5a40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##!pip install spacy\n",
    "##!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "# Load the English preprocessing pipeline\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc918310-c081-48cf-9474-d1b0a95d0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Parse the first reddit post in the dataset\n",
    "parsed_post = nlp(df.selftext.iloc[0])\n",
    "print(parsed_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6cf85-4c62-4f90-8adc-ce3b3c4ce500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each sentence in the parsed post\n",
    "for idx, sentence in enumerate(parsed_post.sents):  \n",
    "    ##In python, .sents is used for \"sentence segmentation\" which is present inside spacy. \n",
    "    print(f'Sentence {idx + 1}')\n",
    "    print(sentence)\n",
    "    print('') #space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e56f9",
   "metadata": {},
   "source": [
    "### 1-4. Preprocessing all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629cb596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2805674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing URLs \n",
    "import re\n",
    "url_pattern = '(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d11084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(token):\n",
    "    \"\"\"Helper function that specifies whether a token is:\n",
    "        - punctuation\n",
    "        - space\n",
    "        - digit\n",
    "    \"\"\"\n",
    "    return token.is_punct or token.is_space or token.is_digit\n",
    "\n",
    "def line_read(df, text_col='selftext'):\n",
    "    \"\"\"Generator function to read in text from df and get rid of line breaks.\"\"\"    \n",
    "    for text in df[text_col]:\n",
    "        yield re.sub(pattern=url_pattern,\n",
    "                     repl=\"\",\n",
    "                     string=text.replace('\\n', ''))\n",
    "\n",
    "def preprocess(df, text_col='selftext', allowed_postags=['NOUN', 'ADJ']):\n",
    "    \"\"\"Preprocessing function to apply to a dataframe.\"\"\"\n",
    "    for parsed in nlp.pipe(line_read(df, text_col), batch_size=1000, disable=[\"tok2vec\", \"ner\"]):\n",
    "        # Gather lowercased, lemmatized tokens\n",
    "        tokens = [token.lemma_.lower() if token.lemma_ != '-PRON-'\n",
    "                  else token.lower_ \n",
    "                  for token in parsed if not clean(token)]\n",
    "        # Remove specific lemmatizations, and words that are not nouns or adjectives\n",
    "        tokens = [lemma\n",
    "                  for lemma in tokens\n",
    "                  if not lemma in [\"'s\",  \"’s\", \"’\"] and not lemma in allowed_postags]\n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "        yield tokens\n",
    "       # Remove url\n",
    "def remove_URL(df, text_col='selftext'):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", text_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d05657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while\n",
    "lemmas = [line for line in preprocess(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0107553-fa22-47b5-9bfa-7afe58c1e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae0d62-b605-4b85-9ccf-8a8d18acc5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index().head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18403226-cabe-41ae-9499-427a54122846",
   "metadata": {},
   "source": [
    "### 1-5.Phrase Modeling with `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61684f-3ef7-47d8-a7ab-0fc1e61bea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# Create bigram and trigram models\n",
    "bigram = Phrases(lemmas, min_count=10, threshold=100)\n",
    "trigram = Phrases(bigram[lemmas], min_count=10, threshold=50)  \n",
    "bigram_phraser = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "# Form trigrams\n",
    "trigrams = [trigram_phraser[bigram_phraser[doc]] for doc in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac415e-aedf-406b-ab33-f133bcc9cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join each into a string\n",
    "trigrams_joined = [' '.join(trigram) for trigram in trigrams]\n",
    "trigrams_joined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46509ba8-2269-440d-9ab3-e33e90691a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use .keys() to identify the bigrams in the dataset. How many bigrams were identified by the parser?\n",
    "len(bigram_phraser.phrasegrams.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f754728-8abe-4f5c-a3bf-a3e6ff051dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at biagrams\n",
    "list(bigram_phraser.phrasegrams.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a815d-5212-4317-ba74-4a4b7f596c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at trigrams\n",
    "[trigram for trigram in list(trigram_phraser.phrasegrams.keys()) if trigram.count('_') == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c691a8ee-1857-4ac8-9c1f-187cb118fb7a",
   "metadata": {},
   "source": [
    "### 1-6. Save the file after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf56a767-36f8-4a1e-b2f9-c2ee9bf4a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting next to selftext column\n",
    "df.insert(loc=7, column='lemmas', value=trigrams_joined)\n",
    "# Removing empty rows in lemmas\n",
    "df = df[~df['lemmas'].isin([''])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5156f53e-5072-4eb4-b094-48d8f92a12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02abe2d-3721-4910-971d-a77ac03a992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most frequent lemmas\n",
    "#lemmacount = df['lemmas'].value_counts()\n",
    "## problems: the lemmas in each post is considerred as a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4bd55-07a5-415a-af65-f6a0f6f73d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"lemmacount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = df['selftext']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "lemma_count = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# Use this if your scikit-learn is older\n",
    "# pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f25023",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_count.T.sort_values(by=0, ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c5ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_count[['patriot','patreon','patriarchal','patriarchate','patriotic','patriotism']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to sum the total counts for lemma \"patriot\"\n",
    "total = lemma_count['patriot'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count all lemmas \n",
    "counts = [lemma_count[col].sum() for col in lemma_count]\n",
    "# Julia    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6809d2c",
   "metadata": {},
   "source": [
    "# Here the \"counts\" is the total frequency of a lemma in the whole corpus. Still need to make a new dataframe for the lemma and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count the actual amount of lemmas using TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Settings that you use for count vectorizer will go here\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_df=0.85,\n",
    "#                                    decode_error='ignore',\n",
    "#                                    stop_words='english',\n",
    "#                                    smooth_idf=True,\n",
    "#                                    use_idf=True)\n",
    "\n",
    "# # Fit and transform the texts\n",
    "# tfidf = tfidf_vectorizer.fit_transform(df['lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas_count = tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ee545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lemmas_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas_count_df = pd.DataFrame.from_dict(lemmas_count, orient='index',columns=['frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be22faf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lemmas_count_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas_count_df.sort_values(by=['frequency'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###rank the count results but didn't get ranked correctly?\n",
    "#lemmas_count_df.rank(axis=0,method='max',numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40128c0f-1221-47bc-84e1-ff9bce565b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to new csv\n",
    "df.to_csv('ukraine_lemmas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1471acc6-8ecc-443a-8fbe-48f4cf4c7c89",
   "metadata": {},
   "source": [
    "### For the next steps, make sure to use the file: 'ukraine_lemmas.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360dd5f6-d48e-4417-8556-8d574ff1e8e7",
   "metadata": {},
   "source": [
    "## 2. Exploring Texts \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb3118-1c93-42f1-baee-3257d5815084",
   "metadata": {},
   "source": [
    "### 2-1. Diving Deeper into `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5e7b30-1e87-4172-b670-ccb5c54f4913",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ukraine_lemmas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e04df-397b-4dd3-ab12-b8ad86fcc7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab0901-2c06-4d78-bfeb-95972d36a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by highest scores\n",
    "df.sort_values(by=['score'], ascending=False)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc65ef9e-5bce-4887-a0ba-b61748939bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows with a score higher than 500\n",
    "df_top = df.loc[df['score'] >= 500, :]\n",
    "len(df_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cd4387-8692-43e1-acdd-625c2110f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique value counts for a column\n",
    "df.flair_text.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e44a11-cf7d-4d93-bdb9-e747419ceb34",
   "metadata": {},
   "source": [
    "### 2-2 Type-token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7d00a-28ea-452e-a9cb-61386986d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the TTR\n",
    "\n",
    "def type_token_ratio(tokens):\n",
    "    \"\"\"Calculates type-token ratio on tokens.\"\"\"\n",
    "    numTokens = len(tokens)\n",
    "    numTypes = len(set(tokens))\n",
    "    return numTypes / numTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3265b219-6865-444e-9088-649adee463e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over the first 10 lemmatized submissions into dataframe\n",
    "\n",
    "for text in df['lemmas'][:10]:\n",
    "    tokens = text.split()\n",
    "    print('Text:\\n', text)\n",
    "    print('TTR:', type_token_ratio(tokens), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c67410-192b-44ee-a144-8c512acfafc2",
   "metadata": {},
   "source": [
    "### 2-3 Processing and Analyzing Language with `Text()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7cda1-f538-4adb-8e77-e04b71ffce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if you do not have nltk installed\n",
    "##!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fbaea6-9b0b-4c28-81d8-a83fe359bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for idx, row in enumerate(df['lemmas']):\n",
    "    # Notice that we put all tokens in the same list\n",
    "    tokens.extend(row.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefa0af-c8ad-4557-9799-e58769379eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.text import Text\n",
    "\n",
    "##aita_tokens = Text(tokens)\n",
    "ukraine_tokens = Text(tokens) ##MJ: I changed the name to make it corresponding to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6aa009-f419-4610-9d48-e866a33b02ac",
   "metadata": {},
   "source": [
    "### Concordances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92ad05-b5a6-46bc-991b-a136668962b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine_tokens.collocation_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109ee96-7b93-4635-933f-683852a5d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change input arguments\n",
    "ukraine_tokens.collocation_list(num=30, window_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ff5e8-247a-474b-a30f-3c2090205c64",
   "metadata": {},
   "source": [
    "### Word Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5185bce-1517-4dfa-95d7-8855690fe0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine_tokens.dispersion_plot([\"stay_strong\", \"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96c6c06-ada2-4bfa-962c-fa75d2bdd65e",
   "metadata": {},
   "source": [
    "### Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f9cff-fb3c-40f5-a969-9bfe5834d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine_tokens.similar('partner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a54c7-5e98-43c0-bc39-217ef809e1bf",
   "metadata": {},
   "source": [
    "### Common Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b9523-7049-48e7-b6a8-1ae6cde872b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine_tokens.common_contexts(['Ukrainian', 'War'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5da1c-7afb-4e80-9cfc-66808a091657",
   "metadata": {},
   "source": [
    "## 2.4 Incorporating Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b4b273-9f4a-4b76-81b8-fc108177b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new colum with date and time\n",
    "df.insert(loc=3, column='created_datetime', value=pd.to_datetime(df['created'], unit='s'))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b11a55-d78a-4aed-8e17-76308450b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new variables years \n",
    "years = pd.DatetimeIndex(df['created_datetime']).year\n",
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d4a74-410f-47c7-8875-b467fa4a91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc83cc-773d-449a-aad5-4d423341fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before 2013\n",
    "df_2013 = df.loc[(years <= 2013), :]\n",
    "len(df_2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd9fa8-e482-4fc0-96ea-1d2d0ebcf5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6204f516-3baf-4097-9b2f-a89c6d2e512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013.flair_css_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e2499-fb0e-4281-964b-ea5ec09692a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after 2013 & before 2022\n",
    "df_b2022 = df.loc[(years <= 2022) & (years >=2013), :]\n",
    "len(df_b2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c0193-1954-4091-9393-46cb7b6b504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a13aa-d195-483f-8d22-de2beff32b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b2022.flair_css_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52fbef4-c3a8-4382-b987-bed7c22792b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after 2022\n",
    "df_a2022 = df.loc[(years >= 2022), :]\n",
    "len(df_a2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e51af6-05f3-4bcb-a9af-5949b1da07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886557a0-ffea-4e8c-b7a9-5b98e8ea0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a2022.flair_css_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb87bb14-d9c3-457d-afd3-2ba399119c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data ###3 different bars + normalized(count -> proportion)\n",
    "#before 2013\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "#sns.set(rc={'figure.figsize': (7, 5)})\n",
    "\n",
    "#p = sns.countplot(\n",
    "#    data=df_2013,\n",
    "#    x=\"flair_css_class\",) ##it is empty\n",
    "\n",
    "#plt.xticks(rotation=70)\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1cc966-635e-4aa6-b3dc-77f854727fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after 2013 & before 2022\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "sns.set(rc={'figure.figsize': (7, 5)})\n",
    "\n",
    "p = sns.countplot(\n",
    "    data=df_b2022,\n",
    "    x=\"flair_css_class\",)\n",
    "\n",
    "\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d6c4e-de60-485d-a24b-823bedbdad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after 2022\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "sns.set(rc={'figure.figsize': (7, 5)})\n",
    "\n",
    "p = sns.countplot(\n",
    "    data=df_a2022,\n",
    "    x=\"flair_css_class\",)\n",
    "\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a7018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can save this dataframe as a cvs file so we don't have to run it everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d887d91-2885-4cc3-86f5-022dfc5cef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013.to_csv('df_2013.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f332e-bd35-4b57-9c81-f844882c499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b2022.to_csv('df_b2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5779808-924f-483b-81eb-9ce5b93be636",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a2022.to_csv('df_a2022.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff5dd1",
   "metadata": {},
   "source": [
    "# 3. Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d151a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1984d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine = pd.read_csv('ukraine_lemmas.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec6e469-4531-4542-a58b-e271bdcbcc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce898ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine['lemmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63fdb3-d057-4e2a-a861-e1d34c94281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of word counts (what we just did with CountVectorizer), and immediately transform them into TF-IDF values.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85,\n",
    "                                   decode_error='ignore',\n",
    "                                   stop_words='english',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True,\n",
    "                                  )\n",
    "\n",
    "# Fit and transform the texts\n",
    "tfidf = tfidf_vectorizer.fit_transform(ukraine['lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac033e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place TF-IDF values in a DataFrame\n",
    "df_tfidf = pd.DataFrame(tfidf.todense(), columns=tfidf_vectorizer.get_feature_names_out().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_vectorizer.get_feature_names()\n",
    "#print(tfidf_vectorizer.vocabulary_)\n",
    "### not sure whetehr they are helpful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b986c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b5aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5c335-29e5-43c2-bee3-397c3d54c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest \"average\" TF-IDF across documents\n",
    "df_tfidf.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average value of tfidf\n",
    "df_tfidf.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the tfidf value of the first post submission\n",
    "df_tfidf.iloc[0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd9380",
   "metadata": {},
   "outputs": [],
   "source": [
    "ukraine['selftext'].iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017a677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the tfidf value of the 10th post submission\n",
    "df_tfidf.iloc[9].sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ce497-83aa-4073-acc0-1ec586680823",
   "metadata": {},
   "source": [
    "## TF-IDF before 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1326f2-0e21-492f-987b-9e1c15d1d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10baeb1f-6a91-40cb-9d6d-6df2694c46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('data') #this will vary by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f4c9b-3180-47e4-bd67-4a5ffba58b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013 = pd.read_csv('df_2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba4a6f-9920-47f7-9c55-3242646c1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013['lemmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2b3dbc-68a0-41da-9970-d19e20715ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of word counts (what we just did with CountVectorizer), and immediately transform them into TF-IDF values.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85,\n",
    "                                   decode_error='ignore',\n",
    "                                   stop_words='english',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True,\n",
    "                                  )\n",
    "\n",
    "# Fit and transform the texts\n",
    "tfidf_2013 = tfidf_vectorizer.fit_transform(df_2013['lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff23b9-4aa0-418e-9f9c-ce204da2336c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf_2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4faf7-9cff-4a1e-a442-c8e7b73ae324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place TF-IDF values in a DataFrame\n",
    "df_tfidf_2013 = pd.DataFrame(tfidf_2013.todense(), columns=tfidf_vectorizer.get_feature_names_out().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852ed09-6a9d-4a10-9348-749759b4e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest \"average\" TF-IDF across documents\n",
    "df_tfidf_2013.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d473ebce-6b8b-4955-af49-ec708240d9bd",
   "metadata": {},
   "source": [
    "## TF-IDF 2014-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456ab709-b435-4a00-8d4b-9d7715f3cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b2022 = pd.read_csv('df_b2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a658b9-2b57-49d7-853c-29bb980e470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b2022['lemmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be10d27-a6c5-48b1-b7c9-9468bccb240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of word counts (what we just did with CountVectorizer), and immediately transform them into TF-IDF values.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85,\n",
    "                                   decode_error='ignore',\n",
    "                                   stop_words='english',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True,\n",
    "                                  )\n",
    "\n",
    "# Fit and transform the texts\n",
    "tfidf_b2022 = tfidf_vectorizer.fit_transform(df_b2022['lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e5d73-555d-418e-8113-d297fa4267d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_b2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48198e64-e9c2-4268-a544-1c340f004c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place TF-IDF values in a DataFrame\n",
    "df_tfidf_b2022 = pd.DataFrame(tfidf_b2022.todense(), columns=tfidf_vectorizer.get_feature_names_out().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d5bf3-f06f-472f-9ac4-199171ae81b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest \"average\" TF-IDF across documents\n",
    "df_tfidf_b2022.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed7907-8031-4ba1-9086-79376623bd8f",
   "metadata": {},
   "source": [
    "## TF-IDF after 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace06ab4-9a62-43ca-a15d-a48bd2b7fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a2022 = pd.read_csv('df_a2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901c3c2-8f90-4a72-98cf-f838832be2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a2022['lemmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd9339-e355-41ba-850e-ec66a245370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of word counts (what we just did with CountVectorizer), and immediately transform them into TF-IDF values.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85,\n",
    "                                   decode_error='ignore',\n",
    "                                   stop_words='english',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True,\n",
    "                                  )\n",
    "\n",
    "# Fit and transform the texts\n",
    "tfidf_a2022 = tfidf_vectorizer.fit_transform(df_a2022['lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935abdd8-9beb-4a5f-8116-5214b4d73f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_a2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d927a-ed6f-4d0d-9101-5b5a6e42a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place TF-IDF values in a DataFrame\n",
    "df_tfidf_a2022 = pd.DataFrame(tfidf_a2022.todense(), columns=tfidf_vectorizer.get_feature_names_out().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c27ab-234c-4eb3-8661-bc5f4b7bf42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest \"average\" TF-IDF across documents\n",
    "df_tfidf_a2022.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200995b",
   "metadata": {},
   "source": [
    "# 3. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46943bf3",
   "metadata": {},
   "source": [
    "## 3-1. Building Topic Models on Ukraine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec2e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ukraine = pd.read_csv('ukraine_lemmas.csv')\n",
    "X = ukraine['lemmas']\n",
    "# Vectorize, using only the top 5000 TF-IDF values\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "tfidf =  vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc312c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=3, max_iter=20, random_state=1)\n",
    "lda = lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff39b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the fuction\n",
    "def plot_top_words(model, feature_names, n_top_words=10, n_row=1, n_col=3, normalize=False):\n",
    "    \"\"\"Plot the top words for an LDA model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : LatentDirichletAllocation object\n",
    "        The trained LDA model.\n",
    "    feature_names : list\n",
    "        A list of strings containing the feature names.\n",
    "    n_top_words : int\n",
    "        The number of top words to show for each topic.\n",
    "    n_row : int\n",
    "        The number of rows to use in the subplots.\n",
    "    n_col : int\n",
    "        The number of columns to use in the subplots.\n",
    "    normalize : bool\n",
    "        If True, normalizes the topic model weights.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(3 * n_col, 5 * n_row), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    components = model.components_\n",
    "    if normalize:\n",
    "        components = components / components.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_names = vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, token_names, 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d822ac0",
   "metadata": {},
   "source": [
    "- Topic 1: Russia on Ukraine war\n",
    "- Topic 2: Travel/Immigration\n",
    "- Topic 3: Social media/Type of posts\n",
    "\n",
    "- Other Comments: Ukrainian president not listed, how come?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6479ec5",
   "metadata": {},
   "source": [
    "## 3-2. Topic Weights Across Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = lda.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de5a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf.shape)\n",
    "print(topic_distributions.shape)\n",
    "print(topic_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b46dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic topic names\n",
    "columns = [\n",
    "    \"Topic 1\",\n",
    "    \"Topic 2\",\n",
    "    \"Topic 3\"\n",
    "]\n",
    "\n",
    "# Or, choose topics\n",
    "columns = [\n",
    "    \"War\",\n",
    "    \"Travel/Immigration\",\n",
    "    \"Social media/Type of posts\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f043051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = pd.DataFrame(topic_distributions, columns=columns)\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.insert(loc=0, column='text', value=ukraine['selftext'])\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [1, 2, 3]\n",
    "\n",
    "for idx in idxs:\n",
    "    print(topic_df['text'].iloc[idx][:500])\n",
    "    print(topic_df.iloc[idx, 1:])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2268dae-b800-42ff-80ea-bc235748e3c8",
   "metadata": {},
   "source": [
    "## 3-3. Change the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e09bb7-2576-4dea-b9da-938d714a02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ukraine = pd.read_csv('ukraine_lemmas.csv')\n",
    "X = ukraine['lemmas']\n",
    "# Vectorize, using only the top 5000 TF-IDF values\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "tfidf =  vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128b6d3-9d3d-4cd0-9b14-f9139a8616ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=5, max_iter=20, random_state=1)\n",
    "lda = lda.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be2f1e-8201-4149-b16c-dfffd96d9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the fuction\n",
    "def plot_top_words(model, feature_names, n_top_words=10, n_row=1, n_col=5, normalize=False):\n",
    "    \"\"\"Plot the top words for an LDA model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : LatentDirichletAllocation object\n",
    "        The trained LDA model.\n",
    "    feature_names : list\n",
    "        A list of strings containing the feature names.\n",
    "    n_top_words : int\n",
    "        The number of top words to show for each topic.\n",
    "    n_row : int\n",
    "        The number of rows to use in the subplots.\n",
    "    n_col : int\n",
    "        The number of columns to use in the subplots.\n",
    "    normalize : bool\n",
    "        If True, normalizes the topic model weights.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(3 * n_col, 5 * n_row), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    components = model.components_\n",
    "    if normalize:\n",
    "        components = components / components.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2527f5-20ab-4245-9839-1ce5e12bbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_names = vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, token_names, 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0584b497-05c9-451d-99b0-f124991e1613",
   "metadata": {},
   "source": [
    "#Interpretation\n",
    "- Topic 1\n",
    "- Topic 2\n",
    "- Topic 3\n",
    "- Topic 4\n",
    "- Topic 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44babf-266f-4ec1-b897-2d8ac96afb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = lda.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140cd231-7459-468c-a2a2-9a21d9bd97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf.shape)\n",
    "print(topic_distributions.shape)\n",
    "print(topic_distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb74af-0302-4137-9f80-039dec5e5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic topic names\n",
    "columns = [\n",
    "    \"Topic 1\",\n",
    "    \"Topic 2\",\n",
    "    \"Topic 3\",\n",
    "    \"Topic 4\",\n",
    "    \"Topic 5\"\n",
    "]\n",
    "\n",
    "# Or, choose topics\n",
    "columns = [\n",
    "    \"War\",\n",
    "    \"\",\n",
    "    \"\", \n",
    "    \"\",\n",
    "    \"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1014708b-d5c8-4aad-a39e-d2150625b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = pd.DataFrame(topic_distributions, columns=columns)\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0292f4e7-476f-4a27-9bf0-73d50578e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.insert(loc=0, column='text', value=ukraine['selftext'])\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f693e-56b2-4514-81a1-4006fe46cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [1, 2, 3]\n",
    "\n",
    "for idx in idxs:\n",
    "    print(topic_df['text'].iloc[idx][:500])\n",
    "    print(topic_df.iloc[idx, 1:])\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e86b49-edff-4c99-9355-7d360f51b097",
   "metadata": {},
   "source": [
    "## 3-4. Topic Modeling in different phases(before 2013, between 2014-2022), after 2022) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b81a4-5cfa-4d61-81f7-9543a6868c75",
   "metadata": {},
   "source": [
    "### 3-4-1. Topic Modeling before 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232f197-c72e-42de-b87c-9a68f0fa9780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = df_2013['lemmas']\n",
    "# Vectorize, using only the top 5000 TF-IDF values\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "tfidf_2013 =  vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7c313-5139-4035-b5db-8b9cd098c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_2013 = LatentDirichletAllocation(n_components=5, max_iter=20, random_state=0)\n",
    "lda_2013 = lda_2013.fit(tfidf_2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e647c-8d62-43fe-8936-f5ebb5895bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the fuction\n",
    "def plot_top_words(model, feature_names, n_top_words=10, n_row=1, n_col=5, normalize=False):\n",
    "    \"\"\"Plot the top words for an LDA model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : LatentDirichletAllocation object\n",
    "        The trained LDA model.\n",
    "    feature_names : list\n",
    "        A list of strings containing the feature names.\n",
    "    n_top_words : int\n",
    "        The number of top words to show for each topic.\n",
    "    n_row : int\n",
    "        The number of rows to use in the subplots.\n",
    "    n_col : int\n",
    "        The number of columns to use in the subplots.\n",
    "    normalize : bool\n",
    "        If True, normalizes the topic model weights.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(3 * n_col, 5 * n_row), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    components = model.components_\n",
    "    if normalize:\n",
    "        components = components / components.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567ecc6-f30e-40e0-a606-db30e7ed51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_names = vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda_2013, token_names, 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de7fb66-e40a-4536-b98e-e08bf7fffbb2",
   "metadata": {},
   "source": [
    "- Topic 1: Daily life/Travel\n",
    "- Topic 2: \n",
    "- Topic 3: \n",
    "- Comments: No war-related words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce52d9a",
   "metadata": {},
   "source": [
    "### 3-4-1. Topic Modeling between 2014 and 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b43e88a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\moren\\\\OneDrive\\\\Documents\\\\Third Year\\\\Summer 22\\\\Data Science\\\\Data-Science-Social-Justice-main\\\\Project'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c9a5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e9c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a2022 = pd.read_csv('df_a2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ee1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = df_a2022['lemmas']\n",
    "# Vectorize, using only the top 5000 TF-IDF values\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "tfidf_a2022 =  vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "466edddc-602b-4e71-84ed-5937eaea1e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_a2022 = LatentDirichletAllocation(n_components=5, max_iter=20, random_state=0)\n",
    "lda_a2022 = lda_a2022.fit(tfidf_a2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "544bb575-880a-4d3a-b1b9-66d3b4f978a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the fuction\n",
    "def plot_top_words(model, feature_names, n_top_words=10, n_row=1, n_col=5, normalize=False):\n",
    "    \"\"\"Plot the top words for an LDA model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : LatentDirichletAllocation object\n",
    "        The trained LDA model.\n",
    "    feature_names : list\n",
    "        A list of strings containing the feature names.\n",
    "    n_top_words : int\n",
    "        The number of top words to show for each topic.\n",
    "    n_row : int\n",
    "        The number of rows to use in the subplots.\n",
    "    n_col : int\n",
    "        The number of columns to use in the subplots.\n",
    "    normalize : bool\n",
    "        If True, normalizes the topic model weights.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(3 * n_col, 5 * n_row), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    components = model.components_\n",
    "    if normalize:\n",
    "        components = components / components.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1adb48a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m token_names \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplot_top_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_a2022\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mplot_top_words\u001b[1;34m(model, feature_names, n_top_words, n_row, n_col, normalize)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_top_words\u001b[39m(model, feature_names, n_top_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_row\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124;03m\"\"\"Plot the top words for an LDA model.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m        If True, normalizes the topic model weights.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(n_row, n_col, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m n_col, \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m n_row), sharex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m     axes \u001b[38;5;241m=\u001b[39m axes\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     22\u001b[0m     components \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcomponents_\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "token_names = vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda_a2022, token_names, 25)\n",
    "plt.show()\n",
    "#nor working to show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f26e4",
   "metadata": {},
   "source": [
    "### 3-4-3. Topic Modeling after 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2e952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b2022 = pd.read_csv('df_b2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "054dc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X = df_b2022['lemmas']\n",
    "# Vectorize, using only the top 5000 TF-IDF values\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "tfidf_b2022 =  vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13bcd5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_b2022 = LatentDirichletAllocation(n_components=5, max_iter=20, random_state=0)\n",
    "lda_b2022 = lda_b2022.fit(tfidf_b2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b44f8101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the fuction\n",
    "def plot_top_words(model, feature_names, n_top_words=10, n_row=1, n_col=5, normalize=False):\n",
    "    \"\"\"Plot the top words for an LDA model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : LatentDirichletAllocation object\n",
    "        The trained LDA model.\n",
    "    feature_names : list\n",
    "        A list of strings containing the feature names.\n",
    "    n_top_words : int\n",
    "        The number of top words to show for each topic.\n",
    "    n_row : int\n",
    "        The number of rows to use in the subplots.\n",
    "    n_col : int\n",
    "        The number of columns to use in the subplots.\n",
    "    normalize : bool\n",
    "        If True, normalizes the topic model weights.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(3 * n_col, 5 * n_row), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    components = model.components_\n",
    "    if normalize:\n",
    "        components = components / components.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c33bdec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m token_names \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplot_top_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_b2022\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36mplot_top_words\u001b[1;34m(model, feature_names, n_top_words, n_row, n_col, normalize)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_top_words\u001b[39m(model, feature_names, n_top_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_row\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124;03m\"\"\"Plot the top words for an LDA model.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m        If True, normalizes the topic model weights.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(n_row, n_col, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m n_col, \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m n_row), sharex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m     axes \u001b[38;5;241m=\u001b[39m axes\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     22\u001b[0m     components \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcomponents_\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "token_names = vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda_b2022, token_names, 25)\n",
    "plt.show()\n",
    "#not working to show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d3912",
   "metadata": {},
   "source": [
    "# 4. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc7b516",
   "metadata": {},
   "source": [
    "### 4-1. Between 2014 and 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c0f16a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpwd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'data'"
     ]
    }
   ],
   "source": [
    "# Package imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "%pwd\n",
    "os.chdir('data') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c026348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12292, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>created_datetime</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>score</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111782174</td>\n",
       "      <td>t3_1ujvoe</td>\n",
       "      <td>1389030774</td>\n",
       "      <td>2014-01-06 17:52:54</td>\n",
       "      <td>0</td>\n",
       "      <td>MaFi0s0</td>\n",
       "      <td>Speaking Russian in small Ukrainian cities?</td>\n",
       "      <td>I understand half of Ukraine speaks Russian, s...</td>\n",
       "      <td>understand half ukraine speaks russian plan le...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>351</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111982888</td>\n",
       "      <td>t3_1uo6js</td>\n",
       "      <td>1389143126</td>\n",
       "      <td>2014-01-08 01:05:26</td>\n",
       "      <td>0</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Met a Ukrainian girl and was wondering if y'al...</td>\n",
       "      <td>I was just wondering if y'all could teach me s...</td>\n",
       "      <td>wondering teach phrases impress tried learn ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112040900</td>\n",
       "      <td>t3_1upfb8</td>\n",
       "      <td>1389183858</td>\n",
       "      <td>2014-01-08 12:24:18</td>\n",
       "      <td>0</td>\n",
       "      <td>IdiotBrit</td>\n",
       "      <td>Moving to Ukraine</td>\n",
       "      <td>I brought my wife to England a few years ago. ...</td>\n",
       "      <td>brought wife england years_ago actually wife f...</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>654</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113297129</td>\n",
       "      <td>t3_1vgcmh</td>\n",
       "      <td>1389974937</td>\n",
       "      <td>2014-01-17 16:08:57</td>\n",
       "      <td>0</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>American gifts for Ukrainian villagers</td>\n",
       "      <td>Moving to Ukraine soon, and living in small Ru...</td>\n",
       "      <td>moving ukraine soon living small russian speak...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>193</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113801822</td>\n",
       "      <td>t3_1vr61q</td>\n",
       "      <td>1390302825</td>\n",
       "      <td>2014-01-21 11:13:45</td>\n",
       "      <td>0</td>\n",
       "      <td>PocketSandInc</td>\n",
       "      <td>Lets start a collection of all the live stream...</td>\n",
       "      <td>Here's what I have so far:\\n\\n* https://www.yo...</td>\n",
       "      <td>far url url url url url url url url url</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>475</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       idint      idstr     created     created_datetime  nsfw         author  \\\n",
       "0  111782174  t3_1ujvoe  1389030774  2014-01-06 17:52:54     0        MaFi0s0   \n",
       "1  111982888  t3_1uo6js  1389143126  2014-01-08 01:05:26     0      [deleted]   \n",
       "2  112040900  t3_1upfb8  1389183858  2014-01-08 12:24:18     0      IdiotBrit   \n",
       "3  113297129  t3_1vgcmh  1389974937  2014-01-17 16:08:57     0      [deleted]   \n",
       "4  113801822  t3_1vr61q  1390302825  2014-01-21 11:13:45     0  PocketSandInc   \n",
       "\n",
       "                                               title  \\\n",
       "0        Speaking Russian in small Ukrainian cities?   \n",
       "1  Met a Ukrainian girl and was wondering if y'al...   \n",
       "2                                  Moving to Ukraine   \n",
       "3             American gifts for Ukrainian villagers   \n",
       "4  Lets start a collection of all the live stream...   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  I understand half of Ukraine speaks Russian, s...   \n",
       "1  I was just wondering if y'all could teach me s...   \n",
       "2  I brought my wife to England a few years ago. ...   \n",
       "3  Moving to Ukraine soon, and living in small Ru...   \n",
       "4  Here's what I have so far:\\n\\n* https://www.yo...   \n",
       "\n",
       "                                              lemmas  score distinguish  \\\n",
       "0  understand half ukraine speaks russian plan le...      4         NaN   \n",
       "1  wondering teach phrases impress tried learn ho...      0         NaN   \n",
       "2  brought wife england years_ago actually wife f...      9         NaN   \n",
       "3  moving ukraine soon living small russian speak...      1         NaN   \n",
       "4            far url url url url url url url url url     16         NaN   \n",
       "\n",
       "   textlen  num_comments flair_text flair_css_class  \n",
       "0      351            16        NaN             NaN  \n",
       "1      169             2        NaN             NaN  \n",
       "2      654             7        NaN             NaN  \n",
       "3      193             4        NaN             NaN  \n",
       "4      475             3        NaN             NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset\n",
    "df_b2022 = pd.read_csv('df_b2022.csv')\n",
    "df.head(3)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "003849aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [lemma.split(' ') for lemma in df['lemmas']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00c440",
   "metadata": {},
   "source": [
    "## 5-1 Constructing a Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "503e0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79f8ab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of cores you have at your disposal\n",
    "cores = multiprocessing.cpu_count()\n",
    "# Word vector dimensionality (how many features each word will be given)\n",
    "n_features = 300\n",
    "# Minimum word count to be taken into account\n",
    "min_word_count = 10\n",
    "# Number of threads to run in parallel (equal to your amount of cores)\n",
    "n_workers = cores\n",
    "# Context window size\n",
    "window = 5\n",
    "# Downsample setting for frequent words\n",
    "downsampling = 1e-2\n",
    "# Seed for the random number generator (to create reproducible results)\n",
    "seed = 1 \n",
    "# Skip-gram = 1, CBOW = 0\n",
    "sg = 1\n",
    "epochs = 20\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=trigrams,\n",
    "    workers=n_workers,\n",
    "    vector_size=n_features,\n",
    "    min_count=min_word_count,\n",
    "    window=window,\n",
    "    sample=downsampling,\n",
    "    seed=seed,\n",
    "    sg=sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5fa7714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3361640, 3954780)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(trigrams, total_examples=model.corpus_count, epochs=10)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c0b8f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('aita.emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "353699b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('aita.emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ca75ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5051"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6ccc73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ukraine'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "372edff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.05339877e-01,  1.15733460e-01,  1.54810781e-02,  1.32737949e-01,\n",
       "       -1.07326552e-01, -1.77850440e-01,  8.69322047e-02,  3.04783024e-02,\n",
       "        1.13598727e-01, -9.01125893e-02,  3.02714676e-01, -1.16310060e-01,\n",
       "        3.57329138e-02,  2.26721540e-02, -2.32074693e-01, -2.45371640e-01,\n",
       "        4.36770096e-02, -1.00320481e-01, -2.47257069e-01, -3.10011625e-01,\n",
       "        1.97342187e-01,  1.58299774e-01,  3.23055014e-02,  3.11491378e-02,\n",
       "        1.85619175e-01,  1.79060832e-01,  6.75676099e-04,  1.18636638e-01,\n",
       "       -2.19166115e-01,  1.97160188e-02, -1.69005483e-01, -2.02404425e-01,\n",
       "        5.74063808e-02, -1.08900003e-01, -4.96282950e-02, -4.37690951e-02,\n",
       "        1.78953782e-01, -4.22112644e-01, -6.17015287e-02, -4.01663873e-03,\n",
       "       -1.33598953e-01,  1.33213267e-01,  7.90039822e-02,  7.94901103e-02,\n",
       "        1.11242875e-01, -6.77441210e-02, -1.09021761e-01,  7.81679079e-02,\n",
       "        6.60476554e-03,  1.42786533e-01, -1.10919856e-01,  6.67778263e-03,\n",
       "       -1.72186285e-01, -1.16674267e-01,  8.81152079e-02, -3.67893949e-02,\n",
       "       -5.74130379e-02, -5.41359074e-02, -6.27421364e-02,  1.92294613e-01,\n",
       "        6.72293548e-03,  8.00867379e-02, -3.01261663e-01, -1.15110673e-01,\n",
       "        1.55496355e-02, -1.47766605e-01,  5.82602173e-02, -1.55080892e-02,\n",
       "       -1.45303041e-01, -1.09061435e-01,  6.29749820e-02,  2.42605790e-01,\n",
       "        2.14400738e-01, -3.17660838e-01,  6.54427335e-02, -8.67932662e-02,\n",
       "        1.24764502e-01, -1.35662779e-01, -1.02200620e-01, -7.12117925e-02,\n",
       "       -1.83485188e-02, -6.49309307e-02,  1.30183831e-01,  1.80218101e-01,\n",
       "       -1.94354892e-01, -1.39110118e-01, -9.35329199e-02, -1.78247690e-02,\n",
       "       -1.71975136e-01,  2.42973000e-01,  6.40546530e-02, -8.87020752e-02,\n",
       "       -6.57286048e-02, -2.09670886e-03,  4.07589853e-01,  6.41096532e-02,\n",
       "       -1.65330857e-01, -1.41577125e-01,  1.44111052e-01,  1.56012610e-01,\n",
       "        1.47463486e-01,  3.54845747e-02,  7.26423860e-02,  2.29886711e-01,\n",
       "        1.51308522e-01, -1.21141464e-01,  3.49783003e-02,  1.60084143e-01,\n",
       "       -2.32592434e-01, -1.71053648e-01, -2.39155725e-01, -2.84859121e-01,\n",
       "       -3.68868411e-02,  3.41955662e-01,  1.43595815e-01,  3.56483102e-01,\n",
       "        1.72662705e-01,  3.18069518e-01, -5.54401949e-02, -3.75792012e-02,\n",
       "       -7.16340821e-03, -6.45280406e-02,  1.48928940e-01, -1.32524267e-01,\n",
       "       -1.01249307e-01,  2.02875733e-01, -9.93139893e-02, -5.25251217e-02,\n",
       "       -9.01767910e-02,  1.04572825e-01,  1.52185887e-01, -1.34595826e-01,\n",
       "        5.49157821e-02, -4.10093516e-01, -5.90360574e-02, -5.11702299e-02,\n",
       "        1.88029893e-02, -1.61444992e-01,  1.70447484e-01,  2.96971530e-01,\n",
       "        9.90753472e-02, -5.19962370e-01, -1.12482086e-01,  4.61138003e-02,\n",
       "       -2.22352341e-01,  5.54058840e-03,  6.71971217e-02, -2.17235200e-02,\n",
       "       -2.38124765e-02,  3.01360190e-02,  2.80566484e-01, -9.34945270e-02,\n",
       "       -1.52926281e-01, -7.23593459e-02, -1.61925659e-01,  1.48182675e-01,\n",
       "       -1.42400101e-01, -4.15309817e-02, -1.99744105e-01,  8.38802531e-02,\n",
       "       -1.55894965e-01,  8.09929147e-02,  4.53319075e-03,  9.37513858e-02,\n",
       "       -9.52222571e-03,  4.54714336e-02, -9.38144103e-02,  1.04800291e-01,\n",
       "       -2.73749344e-02,  1.95322797e-01, -2.52122939e-01, -1.30593613e-01,\n",
       "        1.56665668e-01,  1.16878606e-01, -7.27035031e-02, -2.53573656e-01,\n",
       "       -3.37358564e-04, -1.49568006e-01,  1.06729068e-01, -1.33614230e-03,\n",
       "       -7.29229823e-02,  3.46378200e-02,  3.12690660e-02, -2.91573793e-01,\n",
       "       -1.58863455e-01, -1.62367791e-01,  2.41686553e-01, -5.50617017e-02,\n",
       "        9.50484350e-02, -9.30631235e-02,  5.14965318e-02,  1.24821290e-01,\n",
       "       -6.06821887e-02,  1.38998479e-01, -3.26009467e-02,  1.13702066e-01,\n",
       "        1.81872100e-02, -2.14105826e-02,  2.95489170e-02,  1.11201309e-01,\n",
       "       -2.24375725e-02,  1.86795384e-01, -7.12410733e-02, -3.67553122e-02,\n",
       "        5.91011755e-02,  1.59739405e-01, -1.53141230e-01, -4.33105603e-02,\n",
       "       -1.86509147e-01,  8.69029462e-02,  2.13394146e-02,  8.42999741e-02,\n",
       "       -5.63806202e-03,  1.22674711e-01,  2.81362712e-01, -8.86354893e-02,\n",
       "        3.95912901e-02,  3.38512808e-01,  3.27415913e-02, -4.40127365e-02,\n",
       "       -5.13586625e-02, -4.25064564e-02, -1.23162158e-02, -9.02290419e-02,\n",
       "       -7.53438175e-02, -2.58672297e-01,  5.39789945e-02,  2.82954305e-01,\n",
       "        6.59033731e-02,  1.38114154e-01, -4.52487692e-02, -2.17747971e-01,\n",
       "       -1.21080756e-01, -1.94928348e-02, -2.24298611e-02,  3.94487269e-02,\n",
       "        9.68403295e-02,  1.95831880e-01, -1.87746659e-02,  4.50076908e-02,\n",
       "        4.84981667e-03,  3.46676558e-02, -6.02198988e-02,  1.09752662e-01,\n",
       "       -1.59174018e-02,  5.19851558e-02, -1.68256871e-02, -2.07052499e-01,\n",
       "       -1.18148968e-01,  2.51306612e-02, -6.71354309e-02,  1.16050795e-01,\n",
       "       -1.05656330e-02, -8.93554613e-02, -1.80916518e-01, -2.73690015e-01,\n",
       "        9.38013289e-03, -2.80796766e-01, -6.30367221e-03,  1.23316757e-01,\n",
       "        6.22566417e-02,  1.79281533e-02,  1.07454270e-01,  4.19011600e-02,\n",
       "       -4.44414541e-02,  5.80107681e-02,  7.97687769e-02,  1.00630328e-01,\n",
       "       -2.35847943e-02, -1.37086818e-02, -4.47303653e-02,  1.45077720e-01,\n",
       "       -4.98192012e-02,  7.18929917e-02, -1.10562462e-02,  1.01572210e-02,\n",
       "        1.22019894e-01,  1.57560155e-01, -9.59952027e-02,  1.49414673e-01,\n",
       "       -1.38411662e-02, -1.13840200e-01,  1.43939957e-01, -2.13829055e-01,\n",
       "       -8.22403803e-02, -2.43413985e-01, -1.22591175e-01,  5.91409616e-02,\n",
       "        1.79564074e-01, -3.89548168e-02, -1.06837124e-01,  1.34440903e-02,\n",
       "        5.72046870e-03,  3.45817134e-02,  3.05131882e-01,  1.21228263e-01,\n",
       "        2.64013652e-02, -1.53361827e-01,  1.81373969e-01,  2.64352322e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e19e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not sure how far we want to go with this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13434b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
